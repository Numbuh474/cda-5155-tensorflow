{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using tensorflow version: 2.0.0\n",
      "Using input dir: /Volumes/dev/cda-5155-tensorflow/output/set_all with 582 images\n"
     ]
    }
   ],
   "source": [
    "# from openlocationcode import openlocationcode as olc\n",
    "# olc.encode(47.365590, 8.524997)\n",
    "#'8FVC9G8F+6X'\n",
    "\n",
    "# @see datasets https://github.com/tensorflow/datasets\n",
    "# @see https://stackoverflow.com/questions/44416764/loading-folders-of-images-in-tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "import IPython.display as display\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import pathlib\n",
    "\n",
    "import time\n",
    "default_timeit_steps = 1000\n",
    "\n",
    "# data_dir = '/Volumes/dev/cda-5155-tensorflow/output_256px_grouped_grayscale/set_1'\n",
    "data_dir = '/Volumes/dev/cda-5155-tensorflow/output/set_all'\n",
    "validation_dir = '/Volumes/dev/cda-5155-tensorflow/output/set_validation'\n",
    "\n",
    "data_dir = pathlib.Path(data_dir)\n",
    "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "\n",
    "print('Using tensorflow version: {}'.format(tf.__version__))\n",
    "print('Using input dir: {} with {} images'.format(data_dir, image_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['76XVMM24+PV49G', '76XVMM25+J7M76', '76XVMM24+QHV37',\n",
       "       '76XVJMX4+R4VJR', '76XVJMX3+877FW', '76XVJMX4+V4349',\n",
       "       '76XVJMX2+CQFP4', '76XVJMX4+XPHX2', '76XVJMX5+W4424',\n",
       "       '76XVJMX3+FJFV5', '76XVJMX2+9Q99G', '76XVJMX2+CPW45',\n",
       "       '76XVJMX3+C8QQQ', '76XVJMX4+M47M9', '76XVJMX3+FP9FW',\n",
       "       '76XVJMX3+CPQHJ', '76XVJMX3+9GHPW', '76XVJMX4+WC77W',\n",
       "       '76XVMM24+QJCRV', '76XVMM25+84PJ2', '76XVJMX2+FF286',\n",
       "       '76XVJMX4+WGP67', '76XVJMX4+R4VP8', '76XVJMX2+G4Q7X',\n",
       "       '76XVMM24+4G5QJ', '76XVMM25+J3X49', '76XVMM24+7JQVM',\n",
       "       '76XVMM24+6HQCH', '76XVJMX3+7JXHG', '76XVJMX3+C3P3M',\n",
       "       '76XVMM24+MV9R5', '76XVJMX3+C77RF', '76XVMM24+3HRJ8',\n",
       "       '76XVMM24+CJR4X', '76XVJMX4+W47W5', '76XVJMW3+JVRWJ',\n",
       "       '76XVJMX4+XG5GP', '76XVJMX4+R4VWH', '76XVJMX4+M38V4',\n",
       "       '76XVJMX4+WHR3R', '76XVJMX3+9QC65', '76XVJMX4+R4V8J',\n",
       "       '76XVJMX3+9HXC2', '76XVJMX2+9CGRG', '76XVJMX4+XQ9HC',\n",
       "       '76XVMM25+Q2P34', '76XVJMX4+R6JX4', '76XVJMX2+FR826',\n",
       "       '76XVJMX2+CJCWQ', '76XVMM24+JF4HW', '76XVJMX4+XWPJH',\n",
       "       '76XVJMX4+2GW5R', '76XVJMX2+CHXXJ', '76XVJMX4+R4VHF',\n",
       "       '76XVMM25+63R4V', '76XVMM24+JGX4V', '76XVMM25+923GM',\n",
       "       '76XVJMX2+9VM2V'], dtype='<U14')"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLASS_NAMES = np.array([item.name for item in data_dir.glob('*') ])\n",
    "CLASS_NAMES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 1./255 is to convert from uint8 to float32 in range [0,1].\n",
    "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "validation_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.preprocessing.image.ImageDataGenerator at 0x63f63c110>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEPS_PER_EPOCH: 5.0\n",
      "Found 582 images belonging to 58 classes.\n",
      "Found 18 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Define some parameters for the loader:\n",
    "\n",
    "epochs = 8\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "IMG_HEIGHT = 192\n",
    "IMG_WIDTH = 256\n",
    "STEPS_PER_EPOCH = np.ceil(image_count/BATCH_SIZE)\n",
    "print('STEPS_PER_EPOCH: {}'.format(STEPS_PER_EPOCH))\n",
    "\n",
    "train_data_gen = image_generator.flow_from_directory(directory=str(data_dir),\n",
    "                                                     batch_size=BATCH_SIZE,\n",
    "                                                     shuffle=True,\n",
    "                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                     class_mode='binary'\n",
    "                                                    )\n",
    "\n",
    "val_data_gen = validation_image_generator.flow_from_directory(directory=validation_dir,\n",
    "                                                              batch_size=BATCH_SIZE,\n",
    "                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                              # classes = list(CLASS_NAMES)\n",
    "                                                              class_mode='binary'\n",
    "                                                             )\n",
    "\n",
    "def show_batch(image_batch, label_batch):\n",
    "  plt.figure(figsize=(10,10))\n",
    "  for n in range(25):\n",
    "      ax = plt.subplot(5,5,n+1)\n",
    "      plt.imshow(image_batch[n])\n",
    "      plt.title(CLASS_NAMES[label_batch[n]==1][0].title())\n",
    "      plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_batch, label_batch = next(train_data_gen)\n",
    "# show_batch(image_batch, label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ds = tf.data.Dataset.list_files(str(data_dir/'*/*'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'/Volumes/dev/cda-5155-tensorflow/output/set_all/76XVJMX2+G4Q7X/thumb_0285_lat_29.648834_lon_-82.349641.jpg'\n",
      "b'/Volumes/dev/cda-5155-tensorflow/output/set_all/76XVMM25+84PJ2/thumb_0120_lat_29.65084_lon_-82.3421869.jpg'\n",
      "b'/Volumes/dev/cda-5155-tensorflow/output/set_all/76XVJMX4+XQ9HC/thumb_0172_lat_29.649912_lon_-82.343006.jpg'\n",
      "b'/Volumes/dev/cda-5155-tensorflow/output/set_all/76XVJMX2+G4Q7X/thumb_0280_lat_29.648834_lon_-82.349641.jpg'\n",
      "b'/Volumes/dev/cda-5155-tensorflow/output/set_all/76XVJMX2+9VM2V/thumb_0428_lat_29.648454_lon_-82.347841.jpg'\n"
     ]
    }
   ],
   "source": [
    "for f in list_ds.take(5):\n",
    "  print(f.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUTOTUNE: -1\n"
     ]
    }
   ],
   "source": [
    "def get_label(file_path):\n",
    "  # convert the path to a list of path components\n",
    "  parts = tf.strings.split(file_path, os.path.sep)\n",
    "  # The second to last is the class-directory\n",
    "  return parts[-2] == CLASS_NAMES\n",
    "\n",
    "def decode_img(img):\n",
    "  # convert the compressed string to a 3D uint8 tensor\n",
    "  img = tf.image.decode_jpeg(img, channels=3)\n",
    "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "  # resize the image to the desired size.\n",
    "  # return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])\n",
    "  return img\n",
    "\n",
    "def process_path(file_path):\n",
    "  label = get_label(file_path)\n",
    "  # load the raw data from the file as a string\n",
    "  img = tf.io.read_file(file_path)\n",
    "  img = decode_img(img)\n",
    "  return img, label\n",
    "\n",
    "print('AUTOTUNE: {}'.format(AUTOTUNE))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.\n",
    "labeled_ds = list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape:  (192, 256, 3)\n",
      "Label:  [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False  True False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False]\n"
     ]
    }
   ],
   "source": [
    "for image, label in labeled_ds.take(1):\n",
    "  print(\"Image shape: \", image.numpy().shape)\n",
    "  print(\"Label: \", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n",
    "  # This is a small dataset, only load it once, and keep it in memory.\n",
    "  # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
    "  # fit in memory.\n",
    "  if cache:\n",
    "    if isinstance(cache, str):\n",
    "      ds = ds.cache(cache)\n",
    "    else:\n",
    "      ds = ds.cache()\n",
    "\n",
    "  ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "  # Repeat forever\n",
    "  ds = ds.repeat()\n",
    "\n",
    "  ds = ds.batch(BATCH_SIZE)\n",
    "\n",
    "  # `prefetch` lets the dataset fetch batches in the background while the model\n",
    "  # is training.\n",
    "  ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "  return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speedup loading\n",
    "# train_ds = prepare_for_training(labeled_ds)\n",
    "# image_batch, label_batch = next(iter(train_ds))\n",
    "# show_batch(image_batch.numpy(), label_batch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def timeit(ds, steps=default_timeit_steps):\n",
    "  start = time.time()\n",
    "  it = iter(ds)\n",
    "  for i in range(steps):\n",
    "    batch = next(it)\n",
    "    if i%10 == 0:\n",
    "      print('.',end='')\n",
    "  print()\n",
    "  end = time.time()\n",
    "\n",
    "  duration = end-start\n",
    "  print(\"{} batches: {} s\".format(steps, duration))\n",
    "  print(\"{:0.5f} Images/s\".format(BATCH_SIZE*steps/duration))\n",
    "\n",
    "\n",
    "# timeit(train_data_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timeit(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.DatasetV1Adapter"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# tensorflow.python.data.ops.dataset_ops.DatasetV1Adapter\n",
    "# type(labeled_ds.)\n",
    "\n",
    "# image_paths, labels = labeled_ds # load_base_data(...)\n",
    "# epoch_size = len(image_paths)\n",
    "# image_paths = tf.convert_to_tensor(image_paths, dtype=tf.string)\n",
    "# labels = tf.convert_to_tensor(labels)\n",
    "\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "\n",
    "type(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_ds, _ = next(train_data_gen)\n",
    "\n",
    "len(validation_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_21 (Conv2D)           (None, 192, 256, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 96, 128, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 96, 128, 32)       4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 48, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 48, 64, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 24, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 49152)             0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 512)               25166336  \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 25,190,433\n",
      "Trainable params: 25,190,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(16, 3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_train: 582, total_val: 1\n",
      "Epoch 1/8\n",
      "4/4 [==============================] - 40s 10s/step - loss: -275.4389 - accuracy: 0.0022 - val_loss: 0.8518 - val_accuracy: 0.2778\n",
      "Epoch 2/8\n",
      "4/4 [==============================] - 38s 9s/step - loss: -373.3845 - accuracy: 0.0044 - val_loss: 0.8518 - val_accuracy: 0.2778\n",
      "Epoch 3/8\n",
      "4/4 [==============================] - 42s 11s/step - loss: -381.8036 - accuracy: 0.0000e+00 - val_loss: 0.8518 - val_accuracy: 0.2778\n",
      "Epoch 4/8\n",
      "4/4 [==============================] - 33s 8s/step - loss: -356.8634 - accuracy: 0.0051 - val_loss: 0.8518 - val_accuracy: 0.2778\n",
      "Epoch 5/8\n",
      "4/4 [==============================] - 42s 10s/step - loss: -371.4118 - accuracy: 0.0039 - val_loss: 0.8518 - val_accuracy: 0.2778\n",
      "Epoch 6/8\n",
      "4/4 [==============================] - 38s 9s/step - loss: -378.5854 - accuracy: 0.0022 - val_loss: 0.8518 - val_accuracy: 0.2778\n",
      "Epoch 7/8\n",
      "4/4 [==============================] - 44s 11s/step - loss: -362.7569 - accuracy: 0.0020 - val_loss: 0.8518 - val_accuracy: 0.2778\n",
      "Epoch 8/8\n",
      "4/4 [==============================] - 35s 9s/step - loss: -379.2642 - accuracy: 0.0051 - val_loss: 0.8518 - val_accuracy: 0.2778\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_train = 582 # len(train_ds)\n",
    "total_val =  len(val_data_gen)\n",
    "\n",
    "print('total_train: {}, total_val: {}'.format(total_train, total_val))\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch=total_train // BATCH_SIZE,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_data_gen,\n",
    "    validation_steps=1 # total_val // BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
